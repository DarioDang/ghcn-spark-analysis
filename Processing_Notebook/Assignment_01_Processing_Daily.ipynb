{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter session running on `mathmadslinux2p`.\n",
    "\n",
    "You can start your own Jupyter session on `mathmadslinux2p` and open this notebook in Chrome on the MADS Windows server by\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Login to the MADS Windows server using https://mathportal.canterbury.ac.nz/.\n",
    "2. Download or copy this notebook to your home directory.\n",
    "3. Open powershell and run `ssh mathmadslinux2p`.\n",
    "4. Run `start_pyspark_notebook` or `/opt/anaconda3/bin/jupyter-notebook --ip 132.181.129.68 --port $((8000 + $((RANDOM % 999))))`.\n",
    "5. Copy / paste the url provided in the shell window into Chrome on the MADS Windows server.\n",
    "6. Open the notebook from the Jupyter root directory (which is your home directory).\n",
    "7. Run `start_spark()` to start a spark session in the notebook.\n",
    "8. Run `stop_spark()` before closing the notebook or kill your spark application by hand using the link in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def username():\n",
    "    \"\"\"Get username with any domain information removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'<li><a href=\"{sc.uiWebUrl}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username() + \" (jupyter)\"}</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    user = username()\n",
    "    \n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .master(\"spark://masternode2:7077\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{user}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.executor.memory\", f\"{worker_memory}g\")\n",
    "        .config(\"spark.driver.memory\", f\"{master_memory}g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.ui.port\", str(port))\n",
    "        .appName(user + \" (jupyter)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>kda115 (jupyter)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:4700\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.app.name</td><td>kda115 (jupyter)</td></tr><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>spark://masternode2:7077</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>40717</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>mathmadslinux2p.canterbury.ac.nz</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/users/home/kda115/Spark/Assignment/Supplementary_Material_1/Processing_Notebook/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1726369120221</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>app-20240915145841-1087</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>4700</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Dderby.system.home=/tmp/kda115/spark/</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>kda115 (jupyter)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing - Daily "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the pyspark API to defined data types\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Determine the default block sizes ? The size of 2023 / 2024 files in HDFS? How many blocks in 2023 / 2024? What are individuals block sizes for 2023? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728\r\n"
     ]
    }
   ],
   "source": [
    "# The defaults block sizes of HDFS \n",
    "! hdfs getconf -confKey \"dfs.blocksize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://masternode2:9870/fsck?ugi=kda115&files=1&blocks=1&path=%2Fdata%2Fghcnd%2Fdaily%2F2024.csv.gz\r\n",
      "FSCK started by kda115 (auth:SIMPLE) from /192.168.40.11 for path /data/ghcnd/daily/2024.csv.gz at Sun Aug 25 10:38:45 NZST 2024\r\n",
      "\r\n",
      "/data/ghcnd/daily/2024.csv.gz 88831735 bytes, replicated: replication=8, 1 block(s):  OK\r\n",
      "0. BP-700027894-132.181.129.68-1626517177804:blk_1074220563_479763 len=88831735 Live_repl=8\r\n",
      "\r\n",
      "\r\n",
      "Status: HEALTHY\r\n",
      " Number of data-nodes:\t32\r\n",
      " Number of racks:\t\t1\r\n",
      " Total dirs:\t\t\t0\r\n",
      " Total symlinks:\t\t0\r\n",
      "\r\n",
      "Replicated Blocks:\r\n",
      " Total size:\t88831735 B\r\n",
      " Total files:\t1\r\n",
      " Total blocks (validated):\t1 (avg. block size 88831735 B)\r\n",
      " Minimally replicated blocks:\t1 (100.0 %)\r\n",
      " Over-replicated blocks:\t0 (0.0 %)\r\n",
      " Under-replicated blocks:\t0 (0.0 %)\r\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\r\n",
      " Default replication factor:\t4\r\n",
      " Average block replication:\t8.0\r\n",
      " Missing blocks:\t\t0\r\n",
      " Corrupt blocks:\t\t0\r\n",
      " Missing replicas:\t\t0 (0.0 %)\r\n",
      " Blocks queued for replication:\t0\r\n",
      "\r\n",
      "Erasure Coded Block Groups:\r\n",
      " Total size:\t0 B\r\n",
      " Total files:\t0\r\n",
      " Total block groups (validated):\t0\r\n",
      " Minimally erasure-coded block groups:\t0\r\n",
      " Over-erasure-coded block groups:\t0\r\n",
      " Under-erasure-coded block groups:\t0\r\n",
      " Unsatisfactory placement block groups:\t0\r\n",
      " Average block group size:\t0.0\r\n",
      " Missing block groups:\t\t0\r\n",
      " Corrupt block groups:\t\t0\r\n",
      " Missing internal blocks:\t0\r\n",
      " Blocks queued for replication:\t0\r\n",
      "FSCK ended at Sun Aug 25 10:38:45 NZST 2024 in 0 milliseconds\r\n",
      "\r\n",
      "\r\n",
      "The filesystem under path '/data/ghcnd/daily/2024.csv.gz' is HEALTHY\r\n"
     ]
    }
   ],
   "source": [
    "# How many blocksizes are required in daily climate summarize for 2024?\n",
    "! hdfs fsck /data/ghcnd/daily/2024.csv.gz -files -blocks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2024 Climate Summary:\n",
    "1. File Size: 88,831,735 bytes (~88.8 MB)\n",
    "2. Number of Blocks: 1 block.\n",
    "3. Block Size: Since the file size is smaller than the default block size, it fits entirely within a single block."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://masternode2:9870/fsck?ugi=kda115&files=1&blocks=1&path=%2Fdata%2Fghcnd%2Fdaily%2F2023.csv.gz\r\n",
      "FSCK started by kda115 (auth:SIMPLE) from /192.168.40.11 for path /data/ghcnd/daily/2023.csv.gz at Sun Aug 25 10:40:08 NZST 2024\r\n",
      "\r\n",
      "/data/ghcnd/daily/2023.csv.gz 168357302 bytes, replicated: replication=8, 2 block(s):  OK\r\n",
      "0. BP-700027894-132.181.129.68-1626517177804:blk_1074220535_479735 len=134217728 Live_repl=8\r\n",
      "1. BP-700027894-132.181.129.68-1626517177804:blk_1074220536_479736 len=34139574 Live_repl=8\r\n",
      "\r\n",
      "\r\n",
      "Status: HEALTHY\r\n",
      " Number of data-nodes:\t32\r\n",
      " Number of racks:\t\t1\r\n",
      " Total dirs:\t\t\t0\r\n",
      " Total symlinks:\t\t0\r\n",
      "\r\n",
      "Replicated Blocks:\r\n",
      " Total size:\t168357302 B\r\n",
      " Total files:\t1\r\n",
      " Total blocks (validated):\t2 (avg. block size 84178651 B)\r\n",
      " Minimally replicated blocks:\t2 (100.0 %)\r\n",
      " Over-replicated blocks:\t0 (0.0 %)\r\n",
      " Under-replicated blocks:\t0 (0.0 %)\r\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\r\n",
      " Default replication factor:\t4\r\n",
      " Average block replication:\t8.0\r\n",
      " Missing blocks:\t\t0\r\n",
      " Corrupt blocks:\t\t0\r\n",
      " Missing replicas:\t\t0 (0.0 %)\r\n",
      " Blocks queued for replication:\t0\r\n",
      "\r\n",
      "Erasure Coded Block Groups:\r\n",
      " Total size:\t0 B\r\n",
      " Total files:\t0\r\n",
      " Total block groups (validated):\t0\r\n",
      " Minimally erasure-coded block groups:\t0\r\n",
      " Over-erasure-coded block groups:\t0\r\n",
      " Under-erasure-coded block groups:\t0\r\n",
      " Unsatisfactory placement block groups:\t0\r\n",
      " Average block group size:\t0.0\r\n",
      " Missing block groups:\t\t0\r\n",
      " Corrupt block groups:\t\t0\r\n",
      " Missing internal blocks:\t0\r\n",
      " Blocks queued for replication:\t0\r\n",
      "FSCK ended at Sun Aug 25 10:40:08 NZST 2024 in 1 milliseconds\r\n",
      "\r\n",
      "\r\n",
      "The filesystem under path '/data/ghcnd/daily/2023.csv.gz' is HEALTHY\r\n"
     ]
    }
   ],
   "source": [
    "# How many block sizes are required in daily climate summarize for 2023?\n",
    "! hdfs fsck /data/ghcnd/daily/2023.csv.gz -files -blocks "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2023 Climate Summary:\n",
    "1. File Size: 168,357,302 bytes (~168.4 MB)\n",
    "2. Number of Blocks: 2 blocks.\n",
    "3. Block Sizes:\n",
    "    Block 1: 134,217,728 bytes (128 MB).\n",
    "    Block 2: 34,139,574 bytes (~34.1 MB).\n",
    "4. Explanation: The first block is filled to the default block size limit (128 MB), and the remaining data (34.1 MB) is stored in a second block."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: \n",
    "1. 2024 Summary: The 2024 file is small enough to fit within a single block, using 1 block.\n",
    "2. 2023 Summary: The 2023 file exceeds the block size, resulting in 2 blocks being usedâ€”one full block and one partial block.\n",
    "3. Since both files are relatively small, they do not heavily utilize the block capacity, but the 2023 file does require an additional block due to its larger size."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Load and count the number of observation in 2023 and then seperately in 2024? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using pyspark.sql function to infer schema \n",
    "schema_daily = StructType([\n",
    "    StructField(\"Station_ID\", StringType(), False),\n",
    "    StructField(\"DATE\",  StringType(), True),\n",
    "    StructField(\"Element\", StringType(), True),\n",
    "    StructField(\"VALUE\", IntegerType(), True),\n",
    "    StructField(\"Measurement_Flag\", StringType(), True),\n",
    "    StructField(\"Quality_Flag\", StringType(), True),\n",
    "    StructField(\"Source_Flag\", StringType(), True),\n",
    "    StructField(\"Observation_Time\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations in daily 2023 dataset is 37867272\n"
     ]
    }
   ],
   "source": [
    "# Load the 2023 daily dataset \n",
    "daily_2023 = spark.read.csv(\"hdfs:///data/ghcnd/daily/2023.csv.gz\", schema = schema_daily, header = False)\n",
    "\n",
    "# Count the number of observations in 2023\n",
    "count_daily_2023 = daily_2023.count()\n",
    "\n",
    "print(f\"The number of observations in daily 2023 dataset is {count_daily_2023}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The number of observations in daily 2024 data is 19720790\n"
     ]
    }
   ],
   "source": [
    "# Load the 2024 daily dataset\n",
    "daily_2024 = spark.read.csv(\"hdfs:///data/ghcnd/daily/2024.csv.gz\", schema = schema_daily, header = False)\n",
    "\n",
    "# Count the number of observations in 2024 \n",
    "count_daily_2024 = daily_2024.count()\n",
    "\n",
    "print(f\"The number of observations in daily 2024 data is {count_daily_2024}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.1 How many tasks were executed by each stage of each job?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of partitions: 1\n"
     ]
    }
   ],
   "source": [
    "# Print out the number of partitions\n",
    "num_partitions_2023 = daily_2023.rdd.getNumPartitions()\n",
    "print(f\"Number of partitions: {num_partitions_2023}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There 2 jobs in this executed:\n",
    "1. Read and Count the 2023 daily data (Job 1):\n",
    "    - 2 stages : 1 for reading the data and 1 for counting. \n",
    "2. Read and Count the 2024 daily data (Job 2):\n",
    "    - 2 stages: 1 for reading the data and 1 for counting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Number of Jobs: There are 2 jobs, one for loading and counting the 2023 data and one for loading and counting the 2024 data.\n",
    "2. Number of Stages: Each job has 2 stages: one for reading the data and one for counting, making a total of 4 stages across both jobs.\n",
    "3. Number of Tasks: Each stage has only 1 task because there is only 1 partition in the dataset for both 2023 and 2024."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### B.2 Did the number of tasks executed correspond to the number of blocks in each input? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The number of tasks does not always correspond to the number of blocks in each input. For example, the 2023 dataset was stored across 2 HDFS blocks (128 MB and 34.1 MB). If the block size is configured to be 128 MB in HDFS, this data could typically be split into 2 blocks and will have 2 tasks if each task processes one block. However, due to the file being compressed with Gzip, the situation is different, when Spark encounters a Gzip compressed file, it must be read the entire file as a single unit without paralleizing the read operation across blocks otherwise it would lead to corrupted chunks, because Gzip does not support splitting regardless of how many HDFS blocks it spans (Apache Spark and Data Compression, n.d). As a result, Spark creating only 1 task to process the entire file.                "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cite: (Apache Spark and data compression. (n.d))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C. Load and count the total number of observations in the years 2014 to 2023 (inclusive)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Glob patterns allow you to specify multiple files or directories using wildcard characters. \n",
    "- For example, to load data for the years 2014 through 2023, you can use {2014,2015,2016,...,2023} in the file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of observations from 2014 to 2023: 370803270\n"
     ]
    }
   ],
   "source": [
    "# read the daily dataset from 2014 - 2023 using glob patterns\n",
    "daily_data_2014_2023 = spark.read.csv(\"hdfs:///data/ghcnd/daily/{2014,2015,2016,2017,2018,2019,2020,2021,2022,2023}.csv.gz\",\n",
    "                                     schema = schema_daily, header = False)\n",
    "\n",
    "\n",
    "# Count the number of observations from 2014 - 2023 \n",
    "total_count_2014_2023 = daily_data_2014_2023.count()\n",
    "\n",
    "# Print the result \n",
    "print(f\"Total number of observations from 2014 to 2023: {total_count_2014_2023}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C.1 How many tasks were executed by each stage and how does this number correspond to your input?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This operations involves only one job which is loading and counting the number of observations in the years between 2014 to 2023. As a result, there are 370,803,270 observations from 2014 to 2023.. \n",
    "- Regarding to this job, there are 2 stages which defined as stage 10 and 11 in Figure 5. \n",
    "- Stage 10 processed an input size of 1581.1 MiB which was split into 10 tasks (Figure tasks). The input data from the years 2014 to 2023 was devided into 10 paritions with each parition being processed by a seperate task. The number of tasks directly corresponds to the number of partitions created based on the input size. Hence, there are 10 tasks.            \n",
    "- In stage 11, this stage performed the final count operation which required only 1 task since the data had been aggregated and did not require further partitioning. Therefore, the operation was completed in a single task.\n",
    "- In conclusion, there are 1 job and 2 stages which total 11 tasks, where stage 10 have 10 tasks and stage 11 have 1 task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### D. How many tasks do you think would run in parallel when loading and applying transformations to all years in daily? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### D.1 Can you think of any practical way you could increase this either in Spark or by changing how data stored in HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change in HDFS: According to the 2023 and 2024 daily dataset, the file is stored in HDFS as a Gzip file, which cannot be split, Spark must read the entire file in a single task without parallel processing. By changing the file format to one that supports splitting, such as CSV or plain text, Spark can divide the file into multiple partitions and process them in parallel. This change would reduce the processing time when loading and counting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Change in Spark: Since parallelism is limited when working with Gzip files in Spark by the fact that each Gzip file is processed as a single partition. However, we can still optimize the reading and counting of observations by effectively utilizing the Spark cluster resources. The key to optimizing performance is to process as many Gzip files concurrently as possible, by using  4 executors and 2 cores per executor, we have a total of 8 cores available for task execution. This means Spark can run up to 8 tasks in parallel. Since each Gzip file is processed as a single task, we can process 8 Gzip files concurrently. According to Figure 7, Spark assigned 8 of these files to be processed in parallel across the 8 available cores at launch time 10:16:53. Once these 8 tasks complete, the remaining 2 files are processed by the same 8 cores in 10:17:09 launch time.  As a result, by ensuring that all 8 cores are fully utilized, we can reduce the overall processing time.. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>kda115 (jupyter)</code> is under the completed applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
